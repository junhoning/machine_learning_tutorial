{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip\n",
    "LOG_DIR = './folder_to_save_graph_3' # Here you have to put your log directory\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from glob import glob\n",
    "import shutil \n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/'\n",
    "train_dir = data_dir + 'train/train/'\n",
    "test_dir = data_dir + 'test/test'\n",
    "\n",
    "input_shape = (32, 32, 3)\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "num_epochs = 20\n",
    "data_augmentation = True\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "# base_model = VGG16(weights='imagenet',include_top=False)  # 우측 setting에서 internet을 켜야함\n",
    "\n",
    "# net = base_model.output\n",
    "# net = layers.GlobalAveragePooling2D()(net)\n",
    "# net = layers.Dense(1024,activation='relu')(net)\n",
    "# net = layers.Dense(1024,activation='relu')(net)\n",
    "# net = layers.Dense(512,activation='relu')(net)\n",
    "# net = layers.Dense(2,activation='softmax')(net)\n",
    "\n",
    "# model = tf.keras.Model(inputs=base_model.input,outputs=net)\n",
    "\n",
    "# model.layers\n",
    "\n",
    "# for layer in model.layers[:20]:\n",
    "#     layer.trainable=False\n",
    "# for layer in model.layers[20:]:\n",
    "#     layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "inputs = layers.Input(input_shape)\n",
    "net = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n",
    "net = layers.Conv2D(64, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(64, (3, 3), padding='same')(net)\n",
    "net = layers.BatchNormalization()(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "\n",
    "net = layers.Conv2D(128, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(128, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(128, (3, 3), padding='same')(net)\n",
    "net = layers.BatchNormalization()(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = layers.Dropout(0.25)(net)\n",
    "\n",
    "net = layers.Conv2D(256, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(256, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(256, (3, 3), padding='same')(net)\n",
    "net = layers.BatchNormalization()(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = layers.Dropout(0.25)(net)\n",
    "\n",
    "net = layers.Conv2D(512, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(512, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(512, (3, 3), padding='same')(net)\n",
    "net = layers.BatchNormalization()(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = layers.Dropout(0.25)(net)\n",
    "\n",
    "net = layers.Conv2D(512, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(512, (3, 3), padding='same')(net)\n",
    "net = layers.Conv2D(512, (3, 3), padding='same')(net)\n",
    "net = layers.BatchNormalization()(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.MaxPooling2D(pool_size=(2, 2))(net)\n",
    "net = layers.Dropout(0.25)(net)\n",
    "\n",
    "# net = layers.GlobalAveragePooling2D()(net)\n",
    "net = layers.Flatten()(net)\n",
    "net = layers.Dense(512)(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "net = layers.Dropout(0.5)(net)\n",
    "net = layers.Dense(num_classes)(net)\n",
    "net = layers.Activation('softmax')(net)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_dir + 'train.csv')\n",
    "df.head()\n",
    "\n",
    "#  (fname, cls), (fname, cls), (fname, cls), (fname, cls) (fname, cls)\n",
    "datasets = []\n",
    "for fname, cls in zip(df['id'], df['has_cactus']):\n",
    "    datasets.append((fname, cls))\n",
    "\n",
    "np.random.shuffle(datasets)\n",
    "train_paths = datasets[:int(len(datasets) * 0.8)]\n",
    "test_paths = datasets[int(len(datasets) * 0.8):]\n",
    "\n",
    "# train_rate = 0.8\n",
    "# np.random.shuffle(datasets)\n",
    "# num_trains = int(len(datasets) * train_rate)\n",
    "\n",
    "# train_paths = datasets[:num_trains]\n",
    "# test_paths = datasets[num_trains:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "def batch_dataset(batch_paths):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for fname, cls in batch_paths:\n",
    "        img_path = os.path.join(train_dir, fname)\n",
    "        image = np.array(Image.open(img_path)) / 255.\n",
    "        label = np.array(np.array([0, 1]) == cls).astype(np.uint8)\n",
    "        batch_images.append(image)\n",
    "        batch_labels.append(label)\n",
    "\n",
    "    batch_images = np.array(batch_images)\n",
    "    batch_labels = np.array(batch_labels)\n",
    "    \n",
    "    return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(data_paths, is_training=True):\n",
    "    global_step = 0\n",
    "    steps_per_epoch = len(data_paths) // batch_size\n",
    "    while True:\n",
    "        step = global_step % steps_per_epoch\n",
    "        images, labels = batch_dataset(data_paths[step*batch_size: (step+1)*batch_size])\n",
    "        datagen.fit(images)\n",
    "        images, labels = next(datagen.flow(images, labels)) if is_training else (images, labels)\n",
    "        global_step += 1\n",
    "        \n",
    "        yield images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "#     tf.keras.callbacks.ModelCheckpoint(\n",
    "#         filepath='my_model.h5', # 저장\n",
    "#         monitor='val_loss',\n",
    "#         save_best_only=True, # 가장 좋은 모델\n",
    "#     ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_acc',\n",
    "        factor=0.1, # 콜백이 호출되면 학습률을 10배로 줄임\n",
    "        patience=2, # 몇 번 확인 후 안 오르면 학습률을 줄임\n",
    "    ),\n",
    "    # `val_loss`가 2번의 에포크에 걸쳐 향상되지 않으면 훈련을 멈춥니다.\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    # `./logs` 디렉토리에 텐서보드 로그를 기록니다.\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard.notebook extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard.notebook\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard.notebook\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    np.random.shuffle(train_paths)\n",
    "    steps_per_epoch = len(train_paths) // batch_size\n",
    "    \n",
    "    history = model.fit_generator(data_gen(train_paths), \n",
    "                                  steps_per_epoch=steps_per_epoch, \n",
    "                                  validation_data=batch_dataset(test_paths[:batch_size]),\n",
    "                                  verbose=1,\n",
    "                                  callbacks=callbacks\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches_per_epoch = len(test_paths) // batch_size\n",
    "model.evaluate_generator(data_gen(test_paths, False), \n",
    "                         steps=5, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(data_gen(test_paths, False))\n",
    "\n",
    "logits = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(np.argmax(logits, -1), np.argmax(label, -1))  # (tn, fp), (fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tn, fp), (fn, tp) = confusion_matrix(np.argmax(logits, -1), np.argmax(label, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = data_dir + 'test/test'\n",
    "test_df = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_df['id'][0]\n",
    "\n",
    "answers = []\n",
    "for fname in tqdm_notebook(test_df['id']):\n",
    "    path = os.path.join(test_dir, fname)\n",
    "    image = np.array(Image.open(path)) / 255.\n",
    "    image = np.expand_dims(image, 0)\n",
    "    logit = model.predict(image)\n",
    "    answers.append(logit[0])\n",
    "\n",
    "submit_data = {'id': test_df['id'],\n",
    "               'has_cactus': np.argmax(answers, -1)}\n",
    "\n",
    "submit_df = pd.DataFrame(submit_data)\n",
    "submit_df.to_csv('samplesubmission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
