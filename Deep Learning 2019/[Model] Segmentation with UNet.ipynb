{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부 데이터 불러오기 및 관리 \n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# 이미지 데이터 처리 \n",
    "import numpy as np  # 행렬 및 수학\n",
    "from PIL import Image  # 이미지 처리\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt  # 시각화\n",
    "\n",
    "# Framework\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers  # layer 구현\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # 전처리\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_paths = glob('VOC2007/SegmentationClass/*.png')\n",
    "\n",
    "lbl_path = label_paths[0]\n",
    "\n",
    "label = np.array(Image.open(lbl_path))\n",
    "label = np.where(label==255, 0, label)\n",
    "\n",
    "heights = []\n",
    "widths = []\n",
    "\n",
    "unique_classes = []\n",
    "for path in label_paths:\n",
    "    label = np.array(Image.open(path))\n",
    "    for lbl in np.unique(label):\n",
    "        if lbl not in unique_classes:\n",
    "            unique_classes.append(lbl)\n",
    "    if label.shape[0] not in heights:\n",
    "        heights.append(label.shape[0])\n",
    "    if label.shape[1] not in widths:\n",
    "        widths.append(label.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 255]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_classes.sort()\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343.8333333333333, 381.9230769230769)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(heights), np.mean(widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.7\n",
    "\n",
    "num_classes = 21\n",
    "input_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob('VOC2007/JPEGImages/*.jpg')\n",
    "label_paths = glob('VOC2007/SegmentationClass/*.png')\n",
    "\n",
    "test_rate = 0.8\n",
    "train_images = image_paths[:int(test_rate * len(image_paths))]\n",
    "test_images = image_paths[int(test_rate * len(image_paths)):]\n",
    "\n",
    "train_labels = label_paths[:int(test_rate * len(label_paths))]\n",
    "test_labels = label_paths[int(test_rate * len(label_paths)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4008, 337)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images), len(train_labels)  # 파일 갯수가 달라서 없는 건 제거 할 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = train_labels[0]\n",
    "\n",
    "image_path = label_path.replace(\"SegmentationClass\", \"JPEGImages\").replace(\"png\", \"jpg\")\n",
    "if image_path not in train_images:\n",
    "    print(image_path)  # 없음이 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Load Data (image, label)\n",
    "def get_label(label_path):\n",
    "    label_pil = Image.open(label_path)\n",
    "    label = np.array(label_pil)\n",
    "    label = np.where(label==255, 0, label)  # 255는 필요없어서 제거 \n",
    "    \n",
    "    # Resize Label\n",
    "    label = cv2.resize(label, input_shape[:2])\n",
    "\n",
    "    label_onehot = to_categorical(label, num_classes)\n",
    "\n",
    "    return label_onehot  # shape: (281, 500, 21)\n",
    "\n",
    "def get_data(label_path):\n",
    "    image_path = label_path.replace(\"SegmentationClass\", \"JPEGImages\").replace(\"png\", \"jpg\")\n",
    "    image_pil = Image.open(image_path)\n",
    "    image = np.array(image_pil)\n",
    "    \n",
    "    label = get_label(path)\n",
    "    \n",
    "    # Resize image\n",
    "    image = cv2.resize(image, input_shape[:2])\n",
    "\n",
    "    return image.astype(np.float32)/255, label.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dataset\n",
    "def make_batch(batch_paths):\n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for path in batch_paths:\n",
    "        image, label = get_data(path)\n",
    "        batch_images.append(image)\n",
    "        batch_labels.append(label)\n",
    "\n",
    "    batch_images = np.array(batch_images)\n",
    "    batch_labels = np.array(batch_labels)\n",
    "    \n",
    "    return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(data_paths, is_training=True):\n",
    "    global_step = 0\n",
    "    steps_per_epoch = len(data_paths) // batch_size\n",
    "    while True:\n",
    "        step = global_step % steps_per_epoch\n",
    "        if step == 0:\n",
    "            np.random.shuffle(data_paths)        \n",
    "        images, labels = make_batch(data_paths[step*batch_size: (step+1)*batch_size])\n",
    "        global_step += 1\n",
    "        yield images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\June\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\June\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Build UNet\n",
    "inputs = layers.Input(input_shape)\n",
    "conv1 = layers.Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "conv1 = layers.BatchNormalization()(conv1)\n",
    "conv1 = layers.Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "conv1 = layers.BatchNormalization()(conv1)\n",
    "conv1 = layers.Activation(\"relu\")(conv1)\n",
    "pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = layers.Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "conv2 = layers.BatchNormalization()(conv2)\n",
    "conv2 = layers.Activation(\"relu\")(conv2)\n",
    "conv2 = layers.Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "conv2 = layers.BatchNormalization()(conv2)\n",
    "conv2 = layers.Activation(\"relu\")(conv2)\n",
    "pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = layers.Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "conv3 = layers.BatchNormalization()(conv3)\n",
    "conv3 = layers.Activation(\"relu\")(conv3)\n",
    "conv3 = layers.Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "conv3 = layers.BatchNormalization()(conv3)\n",
    "conv3 = layers.Activation(\"relu\")(conv3)\n",
    "pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "conv4 = layers.Conv2D(512, 3, padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = layers.BatchNormalization()(conv4)\n",
    "conv4 = layers.Activation(\"relu\")(conv4)\n",
    "conv4 = layers.Conv2D(512, 3, padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "conv4 = layers.BatchNormalization()(conv4)\n",
    "conv4 = layers.Activation(\"relu\")(conv4)\n",
    "drop4 = layers.Dropout(drop_rate)(conv4)\n",
    "pool4 = layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = layers.Conv2D(1024, 3, padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = layers.BatchNormalization()(conv5)\n",
    "conv5 = layers.Activation(\"relu\")(conv5)\n",
    "conv5 = layers.Conv2D(1024, 3, padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "conv5 = layers.BatchNormalization()(conv5)\n",
    "conv5 = layers.Activation(\"relu\")(conv5)\n",
    "drop5 = layers.Dropout(drop_rate)(conv5)\n",
    "\n",
    "up6 = layers.Conv2DTranspose(1024, 2, padding='same', strides=(2, 2))(drop5)\n",
    "merge6 = layers.concatenate([drop4, up6], axis=3)\n",
    "conv6 = layers.Conv2D(512, 3, padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = layers.BatchNormalization()(conv6)\n",
    "conv6 = layers.Activation(\"relu\")(conv6)\n",
    "conv6 = layers.Conv2D(512, 3, padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "conv6 = layers.BatchNormalization()(conv6)\n",
    "conv6 = layers.Activation(\"relu\")(conv6)\n",
    "\n",
    "up7 = layers.Conv2DTranspose(512, 2, padding='same', strides=(2, 2))(conv6)\n",
    "merge7 = layers.concatenate([conv3, up7], axis=3)\n",
    "conv7 = layers.Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = layers.BatchNormalization()(conv7)\n",
    "conv7 = layers.Activation(\"relu\")(conv7)\n",
    "conv7 = layers.Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "conv7 = layers.BatchNormalization()(conv7)\n",
    "conv7 = layers.Activation(\"relu\")(conv7)\n",
    "\n",
    "up8 = layers.Conv2DTranspose(256, 2, padding='same', strides=(2, 2))(conv7)\n",
    "merge8 = layers.concatenate([conv2, up8], axis=3)\n",
    "conv8 = layers.Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = layers.BatchNormalization()(conv8)\n",
    "conv8 = layers.Activation(\"relu\")(conv8)\n",
    "conv8 = layers.Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "conv8 = layers.BatchNormalization()(conv8)\n",
    "conv8 = layers.Activation(\"relu\")(conv8)\n",
    "\n",
    "up9 = layers.Conv2DTranspose(128, 2, padding='same', strides=(2, 2))(conv8)\n",
    "merge9 = layers.concatenate([conv1, up9], axis=3)\n",
    "conv9 = layers.Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = layers.BatchNormalization()(conv9)\n",
    "conv9 = layers.Activation(\"relu\")(conv9)\n",
    "conv9 = layers.Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = layers.BatchNormalization()(conv9)\n",
    "conv9 = layers.Activation(\"relu\")(conv9)\n",
    "conv9 = layers.Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "conv10 = layers.Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=conv10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Coefficient\n",
    "def precision(y_true, y_pred):\n",
    "    axes = tuple(range(1, len(y_pred.shape)-1))\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axes)\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axes)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    return K.mean(precision)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    axes = tuple(range(1, len(y_pred.shape)-1))\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axes)\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axes)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    return K.mean(recall)\n",
    "\n",
    "def dice(y_true, y_pred):\n",
    "    epsilon=1e-6\n",
    "    axes = tuple(range(1, len(y_pred.shape)-1)) \n",
    "    numerator = 2. * K.sum(y_pred * y_true, axes)\n",
    "    denominator = K.sum(K.square(y_pred) + K.square(y_true), axes)\n",
    "    \n",
    "    return K.mean(numerator / (denominator + epsilon))\n",
    "\n",
    "def soft_dice_loss(y_true, y_pred):\n",
    "    epsilon=1e-6\n",
    "    axes = tuple(range(1, len(y_pred.shape)-1)) \n",
    "    numerator = 2. * K.sum(y_pred * y_true, axes)\n",
    "    denominator = K.sum(K.square(y_pred) + K.square(y_true), axes)\n",
    "    \n",
    "    return 1 - K.mean(numerator / (denominator + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(os.getcwd(), 'logs')\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=soft_dice_loss, \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\June\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "24/42 [================>.............] - ETA: 43s - loss: 0.9340 - acc: 0.3444"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-82cb497f5dd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                    \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                    verbose=1)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1189\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(train_labels) // batch_size\n",
    "\n",
    "model.fit_generator(generator=data_gen(train_labels), \n",
    "                   steps_per_epoch=steps_per_epoch,\n",
    "                   epochs=num_epochs,\n",
    "                   callbacks=callbacks,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
